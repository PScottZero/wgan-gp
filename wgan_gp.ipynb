{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWZNVEC68TRU"
      },
      "source": [
        "# Conditional WGAN-GP\n",
        "\n",
        "Created by Paul Scott<br>\n",
        "MSE Computer and Information Science<br>\n",
        "University of Pennsylvania<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vQYdO-0m5JF"
      },
      "source": [
        "# References\n",
        "* https://keras.io/examples/generative/wgan_gp/\n",
        "* https://keras.io/examples/generative/conditional_gan/\n",
        "* https://www.tensorflow.org/tutorials/generative/dcgan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpxERlLHnIeq"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6lmIfFYRzqo"
      },
      "source": [
        "### Install Clean FID\n",
        "(runtime restart needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPgS4PZjCoTP"
      },
      "outputs": [],
      "source": [
        "%pip install clean-fid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_w4NnxDRzqq"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4c3HQ3-9Hlg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import shutil\n",
        "import imageio\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from cleanfid import fid\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, preprocessing, optimizers\n",
        "\n",
        "%matplotlib inline \n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
        "noise_dim, num_classes = 0, 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gPzhchvBxyk"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9feYNXfBw-T"
      },
      "outputs": [],
      "source": [
        "def generate_noise(count, class_labels=[], class_index=None):\n",
        "  noise = tf.random.normal((count, noise_dim))\n",
        "  if len(class_labels) > 0:\n",
        "    noise_classes = class_labels\n",
        "  elif class_index != None:\n",
        "    noise_classes = tf.one_hot([class_index] * count, num_classes)\n",
        "  else:\n",
        "    noise_classes = tf.one_hot(np.random.randint(0, num_classes, size=count), num_classes)\n",
        "  noise_classes = tf.reshape(noise_classes, (-1, num_classes))\n",
        "  noise_with_classes = tf.concat((noise, noise_classes), axis=1)\n",
        "  return noise_with_classes, noise_classes\n",
        "\n",
        "\n",
        "def display_image_grid(images, labels=None, title=None, label_maxlen=14):\n",
        "  fig = plt.figure(figsize=(12, 12))\n",
        "  fig.tight_layout()\n",
        "  fig.subplots_adjust(top=0.92)\n",
        "  if title:\n",
        "    fig.suptitle(title, fontsize=24)\n",
        "  for i in range(len(images)):\n",
        "    grid_size = int(np.ceil(np.sqrt(len(images))))\n",
        "    fig.add_subplot(grid_size, grid_size, i + 1)\n",
        "    if labels:\n",
        "      plt.title(labels[i][:label_maxlen])\n",
        "    plt.imshow(images[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "def make_dir(directory):\n",
        "  if not os.path.isdir(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "\n",
        "def remove_dir(directory):\n",
        "  if os.path.isdir(directory):\n",
        "    shutil.rmtree(directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpEoBH6-sxF9"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij394bBhRzqv"
      },
      "source": [
        "### Load and Standardize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twnkpioI9Hlj"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'us_coins_photograde'\n",
        "\n",
        "# ======== local ========\n",
        "dataset_dir = f'input_images/{dataset_name}'\n",
        "\n",
        "# ======== drive =======\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !unzip drive/MyDrive/datasets/{dataset_name}.zip -d {dataset_name}\n",
        "# dataset_dir = dataset_name\n",
        "\n",
        "batch_size = 16\n",
        "img_dim = 32\n",
        "channels = 3\n",
        "img_shape = (img_dim, img_dim, channels)\n",
        "\n",
        "datagen = preprocessing.image.ImageDataGenerator(\n",
        "  preprocessing_function=lambda x: (x - 127.5) / 127.5,\n",
        ")\n",
        "\n",
        "dataset = datagen.flow_from_directory(\n",
        "  dataset_dir,\n",
        "  target_size=(img_dim, img_dim),\n",
        "  batch_size=batch_size,\n",
        "  interpolation='box',\n",
        ")\n",
        "\n",
        "labels = list(dataset.class_indices.keys())\n",
        "num_classes = len(labels)\n",
        "img_shape_with_class = (img_dim, img_dim, channels + num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHRWjbm1Rzqw"
      },
      "source": [
        "### Display Images from Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "TH1igfqr9Hll",
        "outputId": "ed2ed120-7108-4489-f1cd-8afa1be4d454"
      },
      "outputs": [],
      "source": [
        "num_examples = 64\n",
        "\n",
        "images, example_labels = [], []\n",
        "while len(images) < num_examples:\n",
        "  image_batch, label_batch = dataset.next()\n",
        "  images += list(image_batch)\n",
        "  example_labels += list(np.argmax(label_batch, axis=1))\n",
        "\n",
        "images = images[:num_examples]\n",
        "example_labels = [labels[i] for i in example_labels[:num_examples]]\n",
        "example_labels = None if num_classes == 1 else example_labels\n",
        "\n",
        "display_image_grid(images, example_labels, 'Dataset Examples', 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_g-uLTufvxU"
      },
      "source": [
        "# Create WGAN-GP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVNcWWUtr8b5"
      },
      "source": [
        "### Create Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-I4JesfnYWU",
        "outputId": "92274f47-dbb5-4659-a63e-53ee9ac884c8"
      },
      "outputs": [],
      "source": [
        "noise_dim = 512\n",
        "\n",
        "def generator_block(filters, first_layer):\n",
        "  filters = min(filters, 512)\n",
        "  if first_layer:\n",
        "    block_layers = [layers.Conv2DTranspose(filters=filters, kernel_size=4, strides=4, use_bias=False)]\n",
        "  else:\n",
        "    block_layers = [\n",
        "      layers.UpSampling2D(),\n",
        "      layers.Conv2D(filters=filters, kernel_size=3, padding='same', use_bias=False),\n",
        "    ]\n",
        "  block_layers += [\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(0.2),\n",
        "    layers.Conv2D(filters=filters, kernel_size=3, padding='same', use_bias=False),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(0.2),\n",
        "  ]\n",
        "  return block_layers\n",
        "\n",
        "filters = 2 ** (np.log2(img_dim) + 3)\n",
        "min_filters = 32\n",
        "first_layer = True\n",
        "generator_layers = [layers.Reshape(target_shape=(1, 1, noise_dim + num_classes), input_shape=(noise_dim + num_classes,))]\n",
        "while filters >= min_filters:\n",
        "  generator_layers += generator_block(filters, first_layer)\n",
        "  first_layer = False\n",
        "  filters //= 2\n",
        "generator_layers.append(layers.Conv2D(filters=3, kernel_size=1, padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "generator = models.Sequential(generator_layers)\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZHURvUbsAqi"
      },
      "source": [
        "### Create Critic\n",
        "(similar to discriminator in DCGANs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DzUqI66omnE",
        "outputId": "a6a3908c-b35c-4977-e1ee-4ce5182c7838"
      },
      "outputs": [],
      "source": [
        "def critic_block(filters, last_layer):\n",
        "  filters = min(filters, 512)\n",
        "  block_layers = [\n",
        "    layers.Conv2D(filters=filters, kernel_size=3, padding='same', use_bias=False),\n",
        "    layers.LeakyReLU(0.2),\n",
        "    layers.Conv2D(\n",
        "      filters=min(filters*2, 512) if not last_layer else noise_dim,\n",
        "      kernel_size=3 if not last_layer else 4,\n",
        "      strides=1 if not last_layer else 4,\n",
        "      padding='same' if not last_layer else 'valid',\n",
        "      use_bias=False,\n",
        "    ),\n",
        "    layers.LeakyReLU(0.2),\n",
        "  ]\n",
        "  if not last_layer:\n",
        "    block_layers.append(layers.MaxPool2D())\n",
        "  return block_layers\n",
        "\n",
        "filters = 32\n",
        "max_filters = 2 ** (np.log2(img_dim) + 3)\n",
        "critic_layers = [\n",
        " layers.Conv2D(filters=32, kernel_size=1, padding='same', use_bias=False, input_shape=img_shape_with_class),\n",
        " layers.LeakyReLU(0.2), \n",
        "]\n",
        "while filters <= max_filters:\n",
        "  critic_layers += critic_block(filters, filters == max_filters)\n",
        "  first_layer = False\n",
        "  filters *= 2\n",
        "critic_layers += [\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(1),\n",
        "]\n",
        "\n",
        "critic = models.Sequential(critic_layers)\n",
        "critic.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsgPgGeTfvxX"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QXFcYZGZoDi"
      },
      "source": [
        "### Wasserstein Gradient Penalty Loss\n",
        "\n",
        "$L_C=\\mathbb{E}[C(G(z))]-\\mathbb{E}[C(x)]+\\lambda\\mathbb{E}[(||\\nabla C(i)||_2-1)^2]$\n",
        "\n",
        "$L_G=-\\mathbb{E}[C(G(z))]$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $C$ is the critic\n",
        "\n",
        "* $G$ is the generator\n",
        "\n",
        "* $x$ is sampled from the distribution of real images\n",
        "\n",
        "* $z$ is random noise such that $z\\sim\\mathcal{N}(0,1)$\n",
        "\n",
        "* $i$ is an interpolation between real and fake images defined as $i=\\alpha x-(1-\\alpha)G(z)$ where $\\alpha\\sim\\mathcal{U}(0,1)$\n",
        "\n",
        "* $C(x)$ is the critic's score for an input real image $x$\n",
        "\n",
        "* $G(z)$ is an image generated using noise $z$\n",
        "\n",
        "* $C(G(z))$ is the critic's score for an input fake image $G(z)$\n",
        "\n",
        "* $\\lambda$ is the penalty amount hyperparameter (the code uses $\\lambda=10$)\n",
        "\n",
        "Loss Explanation:\n",
        "\n",
        "* The critic is trying to minimize $L_C$, which is equivalent to maximizing $\\mathbb{E}[C(x)]$ and minimizing $\\mathbb{E}[C(G(z))]$. The critic is trying to give the real images higher scores than the fake images.\n",
        "\n",
        "* The generator is trying to minimize $L_G$, which is equivalent to maximizing $\\mathbb{E}[C(G(z))]$. The generator is trying to trick the critic into giving fake images high scores.\n",
        "\n",
        "* A penalty $\\lambda\\mathbb{E}[(||\\nabla C(i)||_2-1)^2]$ is added to the critic loss to keep the norm of the gradient close to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfsD9p4wRzq0"
      },
      "source": [
        "### Define Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFiuGePsqR1C"
      },
      "outputs": [],
      "source": [
        "num_examples = 64\n",
        "save_freq = 20\n",
        "fid_freq = 20\n",
        "show_labels = True\n",
        "\n",
        "# define directories\n",
        "checkpoint_dir = f'checkpoints/{dataset_name}'\n",
        "metrics_dir = f'metrics/{dataset_name}'\n",
        "output_dir = f'output_images/{dataset_name}'\n",
        "\n",
        "# init metrics lists and generate seeds for images shown at each epoch\n",
        "generator_losses, critic_losses, fid_scores = [], [], []\n",
        "seed, seed_labels = generate_noise(num_examples)\n",
        "\n",
        "# calculate stats needed for FID score\n",
        "if not fid.test_stats_exists(dataset_name, 'clean'):\n",
        "  fid.make_custom_stats(dataset_name, dataset_dir)\n",
        "\n",
        "# configure optimizers and checkpoints\n",
        "generator_optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
        "critic_optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "  generator_optimizer=generator_optimizer,\n",
        "  critic_optimizer=critic_optimizer,\n",
        "  generator=generator,\n",
        "  critic=critic,\n",
        ")\n",
        "\n",
        "def train(dataset, epochs, restore_epoch=0):\n",
        "  make_dir('seeds')\n",
        "  np.save(f'seeds/{dataset_name}_seed.npy', seed)\n",
        "\n",
        "  avg_time_per_epoch = 0\n",
        "  for epoch in range(restore_epoch, epochs):\n",
        "    start = time.time()\n",
        "    should_save = (epoch + 1) % save_freq == 0\n",
        "    should_calc_fid = (epoch + 1) % fid_freq == 0\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "      print(f'\\rEpoch {epoch+1} Progress: {i+1}/{len(dataset)}', end='')\n",
        "      image_batch, labels_batch = dataset.next()\n",
        "      critic_loss = train_critic(image_batch, labels_batch)\n",
        "      generator_loss = train_generator(labels_batch)\n",
        "\n",
        "    generator_losses.append(generator_loss)\n",
        "    critic_losses.append(critic_loss)\n",
        "\n",
        "    make_dir(metrics_dir)\n",
        "\n",
        "    if epoch == 0 or should_calc_fid:\n",
        "      fid = calculate_fid_score()\n",
        "      fid_scores.append(fid)\n",
        "      np.save(f'{metrics_dir}/fid_scores.npy', fid_scores)\n",
        "    elif epoch == restore_epoch:\n",
        "      fid = calculate_fid_score()\n",
        "\n",
        "    np.save(f'{metrics_dir}/generator_losses.npy', generator_losses)\n",
        "    np.save(f'{metrics_dir}/critic_losses.npy', critic_losses)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(epoch + 1)\n",
        "    print(f'Generator Loss: {round(float(generator_loss), 3)} | Critic Loss: {round(float(critic_loss), 3)} | FID: {round(fid, 3)}')\n",
        "\n",
        "    if should_save:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    avg_time_per_epoch = print_time(start, epoch, epochs, restore_epoch, avg_time_per_epoch)\n",
        "\n",
        "\n",
        "def train_critic(real_images, real_labels):\n",
        "  noise, _ = generate_noise(len(real_labels), class_labels=real_labels) # z\n",
        "  with tf.GradientTape() as critic_tape:\n",
        "    fake_images = generator(noise, training=True) # G(z)\n",
        "    \n",
        "    real_images = add_labels_to_images(real_images, real_labels)\n",
        "    fake_images = add_labels_to_images(fake_images, real_labels)\n",
        "\n",
        "    real_output = critic(real_images, training=True) # C(x)\n",
        "    fake_output = critic(fake_images, training=True) # C(G(z))\n",
        "    \n",
        "    critic_loss_unpenalized = calculate_critic_loss(real_output, fake_output) # E[C(G(z))] - E[C(x)]\n",
        "    penalty = calculate_gradient_penalty(real_images, fake_images) # E[(||∇C(i)||₂ - 1)²]\n",
        "    critic_loss = critic_loss_unpenalized + 10.0 * penalty # E[C(G(z))] - E[C(x)] + λ * E[(||∇C(i)||₂ - 1)²]\n",
        "\n",
        "  gradients_of_critic = critic_tape.gradient(critic_loss, critic.trainable_variables)\n",
        "  critic_optimizer.apply_gradients(zip(gradients_of_critic, critic.trainable_variables))\n",
        "  return critic_loss\n",
        "\n",
        "\n",
        "def train_generator(real_labels):\n",
        "  noise, _ = generate_noise(len(real_labels), class_labels=real_labels) # z\n",
        "  with tf.GradientTape() as generator_tape:\n",
        "    fake_images = generator(noise, training=True) # G(z)\n",
        "    fake_images = add_labels_to_images(fake_images, real_labels)\n",
        "    fake_output = critic(fake_images, training=True) # C(G(z))\n",
        "    generator_loss = calculate_generator_loss(fake_output) # -E[C(G(z))]\n",
        "\n",
        "  gradients_of_generator = generator_tape.gradient(generator_loss, generator.trainable_variables)\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  return generator_loss\n",
        "\n",
        "\n",
        "# E[C(G(z))] - E[C(x)]\n",
        "def calculate_critic_loss(real_output, fake_output):\n",
        "  return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "\n",
        "# -E[C(G(z))]\n",
        "def calculate_generator_loss(fake_output):\n",
        "  return -tf.reduce_mean(fake_output)\n",
        "\n",
        "\n",
        "# E[(||∇C(i)||₂ - 1)²]\n",
        "def calculate_gradient_penalty(real_images, fake_images):\n",
        "  alpha = tf.random.normal((len(real_images), 1, 1, 1))\n",
        "  interpolated = alpha * real_images + (1 - alpha) * fake_images # i = α * x + (1 - α) * G(z)\n",
        "\n",
        "  with tf.GradientTape() as penalty_tape:\n",
        "    penalty_tape.watch(interpolated)\n",
        "    interpolated_output = critic(interpolated, training=True)\n",
        "\n",
        "  grads = penalty_tape.gradient(interpolated_output, [interpolated])[0] # ∇C(i)\n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=(1, 2, 3))) # ||∇C(i)||₂\n",
        "  penalty = tf.reduce_mean((norm - 1.0) ** 2) # E[(||∇C(i)||₂ - 1)²]\n",
        "  return penalty\n",
        "\n",
        "\n",
        "def calculate_fid_score():\n",
        "  remove_dir('fid_images')\n",
        "  make_dir('fid_images')\n",
        "  max_images = min(10000, len(dataset) * batch_size)\n",
        "  for batch_num in range(len(dataset)):\n",
        "    generated = generator.predict(generate_noise(batch_size)[0])\n",
        "    for i, image in enumerate(generated):\n",
        "      print(f'\\rGenerated Images For FID: {batch_num * batch_size + i + 1}/{max_images}', end='')\n",
        "      image = Image.fromarray((image * 127.5 + 127.5).astype(np.uint8))\n",
        "      image.save(f'fid_images/{batch_num * batch_size + i}.png')\n",
        "      if batch_num * batch_size + i + 1 == max_images:\n",
        "        break\n",
        "    else:\n",
        "      continue\n",
        "    break\n",
        "  print()\n",
        "  return fid.compute_fid('fid_images', dataset_name=dataset_name, dataset_split='custom', num_workers=0)\n",
        "\n",
        "\n",
        "def add_labels_to_images(images, labels):\n",
        "  labels = tf.repeat(labels, (img_dim * img_dim), axis=1)\n",
        "  labels = tf.reshape(labels, (-1, img_dim, img_dim, num_classes))\n",
        "  return tf.concat((images, labels), axis=3)\n",
        "\n",
        "\n",
        "def generate_and_save_images(epoch):\n",
        "  make_dir(output_dir)\n",
        "  predictions = generator(seed, training=False)\n",
        "  generated_labels = [labels[np.argmax(logit)] for logit in seed_labels]\n",
        "  generated_labels = None if num_classes == 1 or not show_labels else generated_labels\n",
        "  display_image_grid(predictions, generated_labels, f'Images Generated at Epoch {epoch}', 8)\n",
        "  plt.savefig('{}/image_at_epoch_{:04d}.png'.format(output_dir, epoch))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def print_time(epoch_start, epoch, total_epochs, restore_epoch, avg_time_per_epoch):\n",
        "  time_for_epoch = time.time() - epoch_start\n",
        "  epoch_adj = epoch - restore_epoch\n",
        "  avg_time_per_epoch = (avg_time_per_epoch * epoch_adj + time_for_epoch) / (epoch_adj + 1)\n",
        "  remaining_epochs = total_epochs - (epoch + 1)\n",
        "  \n",
        "  time_for_epoch_string = get_time_string(time_for_epoch)\n",
        "  remaining_time_string = get_time_string(remaining_epochs * avg_time_per_epoch)\n",
        "\n",
        "  print(f'Time For Epoch {epoch + 1}: {time_for_epoch_string}')\n",
        "  print(f'Remaining Time: {remaining_time_string}')\n",
        "\n",
        "  return avg_time_per_epoch\n",
        "\n",
        "\n",
        "def get_time_string(total_seconds):\n",
        "  hours = int(total_seconds // 3600)\n",
        "  remainder = total_seconds % 3600\n",
        "  minutes = int(remainder // 60)\n",
        "  seconds = round(remainder % 60, 2)\n",
        "  time_string = ''\n",
        "  if hours > 0:\n",
        "    time_string += f'{hours}h '\n",
        "  if remainder >= 60:\n",
        "    time_string += f'{minutes}m '\n",
        "  time_string += f'{seconds}s'\n",
        "  return time_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BdoQoFWZoDk"
      },
      "source": [
        "### Load Checkpoint\n",
        "(if necessary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EWumTILZoDk"
      },
      "outputs": [],
      "source": [
        "checkpoint_num = 4\n",
        "\n",
        "save_freq = 5\n",
        "fid_freq = 5\n",
        "\n",
        "seed = np.load(f'seeds/{dataset_name}_seed.npy')\n",
        "generator_losses = list(np.load(f'{metrics_dir}/generator_losses.npy'))\n",
        "critic_losses = list(np.load(f'{metrics_dir}/critic_losses.npy'))\n",
        "fid_scores = list(np.load(f'{metrics_dir}/fid_scores.npy'))\n",
        "\n",
        "if checkpoint_num:\n",
        "  generator_losses = generator_losses[:checkpoint_num * save_freq]\n",
        "  critic_losses = critic_losses[:checkpoint_num * save_freq]\n",
        "  fid_scores = fid_scores[:(checkpoint_num * save_freq) // fid_freq + 1]\n",
        "  with open(f'{checkpoint_dir}/checkpoint', 'w') as ckpt:\n",
        "    ckpt.write(\n",
        "      f'''\n",
        "      model_checkpoint_path: \"ckpt-{checkpoint_num}\"\n",
        "      all_model_checkpoint_paths: \"ckpt-{checkpoint_num}\"\n",
        "      '''\n",
        "    )\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLhrdJlgZoDk"
      },
      "source": [
        "### Train WGAN-GP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bYB65urfvxY",
        "outputId": "79cbb72f-03a1-4e19-9614-c1f97ac2342f"
      },
      "outputs": [],
      "source": [
        "train(dataset, 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiOW4wHyFg6u"
      },
      "source": [
        "### Plot Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "ZHAvZkisEwLX",
        "outputId": "4b24b17c-e982-449f-97fe-c3e20ad8b147"
      },
      "outputs": [],
      "source": [
        "def get_poly_curve(y_values):\n",
        "  x = np.arange(len(y_values))\n",
        "  y = np.array(y_values)\n",
        "  curve = np.polyfit(x, y, 2)\n",
        "  poly_y = np.sum([a * x ** (2 - i) for i, a in enumerate(curve)], axis=0)\n",
        "  return x, poly_y\n",
        "\n",
        "# fit curves to losses\n",
        "generator_curve_x, generator_curve_y = get_poly_curve(generator_losses)\n",
        "critic_curve_x, critic_curve_y = get_poly_curve(critic_losses)\n",
        "\n",
        "# create figure for metrics plots\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\n",
        "\n",
        "# plot generator loss\n",
        "ax1.plot(generator_losses)\n",
        "ax1.plot(generator_curve_x, generator_curve_y)\n",
        "ax1.set_title('Generator Loss')\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('generator loss')\n",
        "\n",
        "# plot critic loss\n",
        "ax2.plot(critic_losses)\n",
        "ax2.plot(critic_curve_x, critic_curve_y)\n",
        "ax2.set_title('Critic Loss')\n",
        "ax2.set_xlabel('epoch')\n",
        "ax2.set_ylabel('critic loss')\n",
        "\n",
        "# plot fid scores\n",
        "ax3.plot(np.arange(0, len(fid_scores) * fid_freq, fid_freq), fid_scores)\n",
        "ax3.set_title('FID Score')\n",
        "ax3.set_xlabel('epoch')\n",
        "ax3.set_ylabel('fid')\n",
        "\n",
        "# show plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L87I8ZuNZoDm"
      },
      "source": [
        "### Save Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IaStQZSZoDm",
        "outputId": "f8533270-8e17-4f43-d0ff-1669b3aa7a65"
      },
      "outputs": [],
      "source": [
        "make_dir('saved_generators/')\n",
        "models.save_model(generator, f'saved_generators/{dataset_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcwLF40EZoDm"
      },
      "source": [
        "### Create GIF of Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9oY7nsFfvxZ"
      },
      "outputs": [],
      "source": [
        "skip_size = 1\n",
        "end_epoch = 20\n",
        "\n",
        "with imageio.get_writer(f'gifs/{dataset_name}.gif', mode='I') as writer:\n",
        "  filenames = sorted(glob.glob(f'output_images/{dataset_name}/image*.png'))\n",
        "  for i in range(0, len(filenames), skip_size):\n",
        "    if i == end_epoch:\n",
        "      break\n",
        "    writer.append_data(imageio.imread(filenames[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iGQEjXKZoDn"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfvFzpX1ZoDn"
      },
      "source": [
        "### Load Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuW3vN0yZoDn",
        "outputId": "e5600d9a-36db-41c3-d6d9-c06336443fc7"
      },
      "outputs": [],
      "source": [
        "generator = models.load_model(f'saved_generators/{dataset_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HABiEfupZoDo"
      },
      "source": [
        "### Generate Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hHfK2GpWyOW",
        "outputId": "d9103319-e52c-4a63-ba84-79bfcf1ba2d8"
      },
      "outputs": [],
      "source": [
        "num_examples = 25\n",
        "class_index = None\n",
        "\n",
        "noise, noise_labels = generate_noise(num_examples, class_index=class_index)\n",
        "noise_labels = [labels[np.argmax(label)] for label in noise_labels]\n",
        "noise_labels = None if len(labels) == 1 else noise_labels\n",
        "generated = generator(noise)\n",
        "display_image_grid(generated, noise_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njfz7a3r-BzG"
      },
      "source": [
        "### Explore latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "1-Q1QyiOp-md",
        "outputId": "303050d1-5172-448a-ebef-edc8c17175d5"
      },
      "outputs": [],
      "source": [
        "num_transitions = 7\n",
        "num_examples = 8\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "for i in range(num_examples):\n",
        "  classes = np.random.choice(np.arange(num_classes), size=2)\n",
        "  start_noise = np.concatenate((np.random.normal(size=noise_dim), tf.one_hot(classes[0], num_classes))).reshape((1, noise_dim + num_classes))\n",
        "  end_noise = np.concatenate((np.random.normal(size=noise_dim), tf.one_hot(classes[1], num_classes))).reshape((1, noise_dim + num_classes))\n",
        "  for t in range(num_transitions + 1):\n",
        "    step = t / (num_transitions + 1)\n",
        "    noise = (1 - step) * start_noise + step * end_noise\n",
        "    output = generator.predict(noise).reshape(img_shape)\n",
        "    grid_size = int(np.ceil(np.sqrt(num_examples)))\n",
        "    fig.add_subplot(num_transitions + 1, num_examples, i * (num_transitions + 1) + t + 1)\n",
        "    plt.imshow(output * 0.5 + 0.5)\n",
        "    plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0vQYdO-0m5JF"
      ],
      "machine_shape": "hm",
      "name": "Copy of wgan_gp.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1bdf2eaed62a5898b0a69d4c8adec4b0ea9657cc1f97a559901a9b3e3bb02e72"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
