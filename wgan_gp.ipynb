{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vQYdO-0m5JF"
      },
      "source": [
        "# References\n",
        "* https://keras.io/examples/generative/wgan_gp/\n",
        "* https://github.com/julianzaidi/Project-IFT6266/blob/master/DCGAN/model.py\n",
        "* https://www.tensorflow.org/tutorials/generative/dcgan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpxERlLHnIeq"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "s4c3HQ3-9Hlg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import imageio\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, preprocessing, optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpEoBH6-sxF9"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twnkpioI9Hlj",
        "outputId": "e52d162c-77c6-467e-b384-164fc0efe174"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "img_dim = 128\n",
        "channels = 3\n",
        "img_shape = (img_dim, img_dim, channels)\n",
        "\n",
        "dataset = preprocessing.image_dataset_from_directory(\n",
        "  'C:/Users/8psco/Desktop/ancient_coins/greek_coins',\n",
        "  image_size=(img_dim, img_dim),\n",
        "  batch_size=batch_size,\n",
        "  interpolation='area',\n",
        "  validation_split=10000/42703,\n",
        "  subset='validation',\n",
        "  seed=1234,\n",
        ")\n",
        "dataset = dataset.map(lambda x, _: (x - 127.5) / 127.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "TH1igfqr9Hll",
        "outputId": "db1d0552-009c-4a73-d3d1-015a8992a3f3"
      },
      "outputs": [],
      "source": [
        "num_examples = 36\n",
        "\n",
        "X = []\n",
        "while len(X) < num_examples:\n",
        "  X += list(dataset.as_numpy_iterator().next())\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "for i in range(num_examples):\n",
        "  grid_size = int(np.ceil(np.sqrt(num_examples)))\n",
        "  fig.add_subplot(grid_size, grid_size, i + 1)\n",
        "  plt.imshow(X[i] * 0.5 + 0.5)\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_g-uLTufvxU"
      },
      "source": [
        "# Create WGAN-GP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVNcWWUtr8b5"
      },
      "source": [
        "Create generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-I4JesfnYWU",
        "outputId": "bf47c5a3-e1ef-44fd-aeb3-cfa8d322fa09"
      },
      "outputs": [],
      "source": [
        "noise_dim = 128\n",
        "\n",
        "generator = models.Sequential([\n",
        "  layers.Reshape(target_shape=(1, 1, noise_dim), input_shape=(noise_dim,)),\n",
        "\n",
        "  layers.Conv2DTranspose(filters=512, kernel_size=4, strides=4, use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2DTranspose(filters=512, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "\n",
        "  layers.UpSampling2D(),\n",
        "  layers.Conv2DTranspose(filters=256, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2DTranspose(filters=256, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "\n",
        "  layers.UpSampling2D(),\n",
        "  layers.Conv2DTranspose(filters=128, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2DTranspose(filters=128, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "\n",
        "  layers.UpSampling2D(),\n",
        "  layers.Conv2DTranspose(filters=64, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2DTranspose(filters=64, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "\n",
        "  layers.UpSampling2D(),\n",
        "  layers.Conv2DTranspose(filters=32, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2DTranspose(filters=32, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "\n",
        "  layers.UpSampling2D(),\n",
        "  layers.Conv2DTranspose(filters=16, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2DTranspose(filters=16, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.LeakyReLU(0.2),\n",
        "\n",
        "  layers.Conv2DTranspose(filters=3, kernel_size=1, padding='same', use_bias=False, activation='tanh'),\n",
        "])\n",
        "\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZHURvUbsAqi"
      },
      "source": [
        "Create critic (similar to discriminator in DCGANs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DzUqI66omnE",
        "outputId": "6a5db143-33fe-464d-c823-26d466bbf69e"
      },
      "outputs": [],
      "source": [
        "critic = models.Sequential([\n",
        "  layers.Conv2D(filters=16, kernel_size=1, padding='same',use_bias=False, input_shape=img_shape),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2D(filters=16, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2D(filters=32, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.MaxPool2D(),\n",
        "\n",
        "  layers.Conv2D(filters=32, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2D(filters=64, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.MaxPool2D(),\n",
        "\n",
        "  layers.Conv2D(filters=64, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2D(filters=128, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.MaxPool2D(),\n",
        "\n",
        "  layers.Conv2D(filters=128, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.MaxPool2D(),\n",
        "\n",
        "  layers.Conv2D(filters=256, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2D(filters=512, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.MaxPool2D(),\n",
        "\n",
        "  layers.Conv2D(filters=512, kernel_size=3, padding='same', use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "  layers.Conv2D(filters=noise_dim, kernel_size=4, strides=4, use_bias=False),\n",
        "  layers.LeakyReLU(0.2),\n",
        "\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(1),\n",
        "])\n",
        "critic.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsgPgGeTfvxX"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wasserstein Gradient Penalty Loss (Critic and Generator Losses)\n",
        "\n",
        "$L_C=\\mathbb{E}[C(G(z))]-\\mathbb{E}[C(x)]+\\lambda\\mathbb{E}[(||\\nabla C(i)||_2-1)^2]$\n",
        "\n",
        "$L_G=-\\mathbb{E}[C(G(z))]$\n",
        "\n",
        "Where\n",
        "\n",
        "* $C$ is the critic\n",
        "\n",
        "* $G$ is the generator\n",
        "\n",
        "* $x$ is sampled from the distribution of real images\n",
        "\n",
        "* $z$ is random noise such that $z\\sim\\mathcal{N}(0,1)$\n",
        "\n",
        "* $i$ is an interpolation between real and fake images defined as $i=\\alpha x-(1-\\alpha)G(z)$ where $\\alpha\\sim\\mathcal{U}(0,1)$\n",
        "\n",
        "* $C(x)$ is the critic's score for an input real image $x$\n",
        "\n",
        "* $G(z)$ is an image generated using noise $z$\n",
        "\n",
        "* $C(G(z))$ is the critic's score for an input fake image $G(z)$\n",
        "\n",
        "* $\\lambda$ is the penalty amount hyperparameter (the code uses $\\lambda=10$)\n",
        "\n",
        "Loss Explanations\n",
        "\n",
        "* The critic is trying to minimize $L_C$, which is equivalent to maximizing $\\mathbb{E}[C(x)]$ and minimizing $\\mathbb{E}[C(G(z))]$. The critic is trying to give the real images higher scores than the fake images.\n",
        "\n",
        "* The generator is trying to minimize $L_G$ (which is equivalent to maximizing $\\mathbb{E}[C(G(z))]$). The generator is trying to trick the critic into giving fake images high scores.\n",
        "\n",
        "* A penalty $\\lambda\\mathbb{E}[(||\\nabla C(i)||_2-1)^2]$ is added to the critic loss to prevent the gradient of the critic from growing too large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFiuGePsqR1C"
      },
      "outputs": [],
      "source": [
        "num_examples = 36 # number of generated images to show each epoch\n",
        "\n",
        "generator_optimizer = optimizers.Adam(beta_1=0.0, beta_2=0.99)\n",
        "critic_optimizer = optimizers.Adam(beta_1=0.0, beta_2=0.99)\n",
        "generator_losses, critic_losses = [], []\n",
        "seed = tf.random.normal([num_examples, noise_dim])\n",
        "\n",
        "dataset_name = 'greek_coins'\n",
        "checkpoint_dir = f'checkpoints/{dataset_name}'\n",
        "output_dir = f'output_images/{dataset_name}'\n",
        "losses_dir = f'losses/{dataset_name}'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "  generator_optimizer=generator_optimizer,\n",
        "  critic_optimizer=critic_optimizer,\n",
        "  generator=generator,\n",
        "  critic=critic,\n",
        ")\n",
        "\n",
        "def train(dataset, epochs, restore_epoch=0, save_freq=20):\n",
        "  mkdir('seeds')\n",
        "  np.save(f'seeds/{dataset_name}_seed.npy', seed)\n",
        "\n",
        "  avg_time_per_epoch = 0\n",
        "  for epoch in range(restore_epoch, epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      critic_loss = train_critic(image_batch)\n",
        "      generator_loss = train_generator(len(image_batch))\n",
        "\n",
        "    generator_losses.append(generator_loss)\n",
        "    critic_losses.append(critic_loss)\n",
        "\n",
        "    mkdir(losses_dir)\n",
        "    np.save(f'{losses_dir}/generator_losses.npy', generator_losses)\n",
        "    np.save(f'{losses_dir}/critic_losses.npy', critic_losses)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator, epoch + 1, seed)\n",
        "    print(f'Generator Loss: {round(float(generator_loss), 3)} | Critic Loss: {round(float(critic_loss), 3)}')\n",
        "\n",
        "    if (epoch + 1) % save_freq == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    avg_time_per_epoch = print_time(start, epoch, epochs, restore_epoch, avg_time_per_epoch)\n",
        "\n",
        "\n",
        "def train_critic(real_images):\n",
        "  noise = tf.random.normal([len(real_images), noise_dim]) # z\n",
        "  with tf.GradientTape() as critic_tape:\n",
        "    fake_images = generator(noise, training=True) # G(z)\n",
        "\n",
        "    real_output = critic(real_images, training=True) # C(x)\n",
        "    fake_output = critic(fake_images, training=True) # C(G(z))\n",
        "    \n",
        "    critic_loss_unpenalized = get_critic_loss(real_output, fake_output) # E[C(G(z))] - E[C(x)]\n",
        "    penalty = get_gradient_penalty(real_images, fake_images) # E[(||∇C(i)||₂ - 1)²]\n",
        "    critic_loss = critic_loss_unpenalized + 10.0 * penalty # E[C(G(z))] - E[C(x)] + λ * E[(||∇C(i)||₂ - 1)²]\n",
        "\n",
        "  gradients_of_critic = critic_tape.gradient(critic_loss, critic.trainable_variables)\n",
        "  critic_optimizer.apply_gradients(zip(gradients_of_critic, critic.trainable_variables))\n",
        "  return critic_loss\n",
        "\n",
        "\n",
        "def train_generator(batch_size):\n",
        "  noise = tf.random.normal([batch_size, noise_dim])\n",
        "  with tf.GradientTape() as generator_tape:\n",
        "    fake_images = generator(noise, training=True) # G(z)\n",
        "    fake_output = critic(fake_images, training=True) # C(G(z))\n",
        "    generator_loss = get_generator_loss(fake_output) # -E[C(G(z))]\n",
        "\n",
        "  gradients_of_generator = generator_tape.gradient(generator_loss, generator.trainable_variables)\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  return generator_loss\n",
        "\n",
        "\n",
        "# E[C(G(z))] - E[C(x)]\n",
        "def get_critic_loss(real_output, fake_output):\n",
        "  return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
        "\n",
        "\n",
        "# -E[C(G(z))]\n",
        "def get_generator_loss(fake_output):\n",
        "  return -tf.reduce_mean(fake_output)\n",
        "\n",
        "\n",
        "# E[(||∇C(i)||₂ - 1)²]\n",
        "def get_gradient_penalty(real_images, fake_images):\n",
        "  alpha = tf.random.normal((len(real_images), 1, 1, 1))\n",
        "  interpolated = alpha * real_images + (1 - alpha) * fake_images # i = α * x + (1 - α) * G(z)\n",
        "\n",
        "  with tf.GradientTape() as penalty_tape:\n",
        "    penalty_tape.watch(interpolated)\n",
        "    interpolated_output = critic(interpolated, training=True)\n",
        "\n",
        "  grads = penalty_tape.gradient(interpolated_output, [interpolated])[0] # ∇C(i)\n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=(1, 2, 3))) # ||∇C(i)||₂\n",
        "  penalty = tf.reduce_mean((norm - 1.0) ** 2) # E[(||∇C(i)||₂ - 1)²]\n",
        "  return penalty\n",
        "\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  mkdir(output_dir)\n",
        "  predictions = model(test_input, training=False)\n",
        "  plt.figure(figsize=(12, 12))\n",
        "  grid_size = int(np.ceil(np.sqrt(num_examples)))\n",
        "  for i in range(predictions.shape[0]):\n",
        "    plt.subplot(grid_size, grid_size, i + 1)\n",
        "    plt.imshow(predictions[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.savefig('{}/image_at_epoch_{:04d}.png'.format(output_dir, epoch))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def print_time(epoch_start, epoch, total_epochs, restore_epoch, avg_time_per_epoch):\n",
        "  time_for_epoch = time.time() - epoch_start\n",
        "  epoch_adj = epoch - restore_epoch\n",
        "  avg_time_per_epoch = (avg_time_per_epoch * epoch_adj + time_for_epoch) / (epoch_adj + 1)\n",
        "  remaining_epochs = total_epochs - (epoch + 1)\n",
        "  \n",
        "  time_for_epoch_string = get_time_string(time_for_epoch)\n",
        "  remaining_time_string = get_time_string(remaining_epochs * avg_time_per_epoch)\n",
        "\n",
        "  print(f'Time For Epoch {epoch + 1}: {time_for_epoch_string}')\n",
        "  print(f'Remaining Time: {remaining_time_string}')\n",
        "\n",
        "  return avg_time_per_epoch\n",
        "\n",
        "\n",
        "def get_time_string(total_seconds):\n",
        "  hours = int(total_seconds // 3600)\n",
        "  remainder = total_seconds % 3600\n",
        "  minutes = int(remainder // 60)\n",
        "  seconds = round(remainder % 60, 2)\n",
        "  time_string = ''\n",
        "  if hours > 0:\n",
        "    time_string += f'{hours}h '\n",
        "  if remainder >= 60:\n",
        "    time_string += f'{minutes}m '\n",
        "  time_string += f'{seconds}s'\n",
        "  return time_string\n",
        "\n",
        "\n",
        "def mkdir(directory):\n",
        "  if not os.path.isdir(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "\n",
        "def restore(init_value, npy_file):\n",
        "  if os.path.isfile(npy_file):\n",
        "    return np.load(npy_file)\n",
        "  return init_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Restore if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_num = None\n",
        "\n",
        "seed = np.load(f'seeds/{dataset_name}_seed.npy')\n",
        "generator_losses = list(np.load(f'{losses_dir}/generator_losses.npy'))[:60]\n",
        "critic_losses = list(np.load(f'{losses_dir}/critic_losses.npy'))[:60]\n",
        "\n",
        "if checkpoint_num:\n",
        "  with open(f'{checkpoint_dir}/checkpoint', 'w') as ckpt:\n",
        "    ckpt.write(\n",
        "      f'''\n",
        "      model_checkpoint_path: \"ckpt-{checkpoint_num}\"\n",
        "      all_model_checkpoint_paths: \"ckpt-{checkpoint_num}\"\n",
        "      '''\n",
        "    )\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train WGAN-GP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-bYB65urfvxY",
        "outputId": "bf79ff02-ec60-4fbb-9534-1ad60840e1a1"
      },
      "outputs": [],
      "source": [
        "train(dataset, 300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator_losses = np.array(generator_losses)\n",
        "critic_losses = np.array(critic_losses)\n",
        "\n",
        "def get_poly_curve(x=[], y=[], deg=2):\n",
        "  if len(x) == 0: x = np.arange(len(y))\n",
        "  curve = np.polyfit(x, y, deg)\n",
        "  y = np.sum([a * x ** (deg - i) for i, a in enumerate(curve)], axis=0)\n",
        "  return x, y\n",
        "\n",
        "generator_curve_x, generator_curve_y = get_poly_curve(y=generator_losses)\n",
        "critic_curve_x, critic_curve_y = get_poly_curve(y=critic_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiOW4wHyFg6u"
      },
      "source": [
        "Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHAvZkisEwLX"
      },
      "outputs": [],
      "source": [
        "# plot generator loss\n",
        "plt.plot(generator_losses)\n",
        "plt.plot(generator_curve_x, generator_curve_y)\n",
        "plt.title('Generator Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('generator loss')\n",
        "plt.show()\n",
        "\n",
        "# plot critic loss\n",
        "plt.plot(critic_losses)\n",
        "plt.plot(critic_curve_x, critic_curve_y)\n",
        "plt.title('Critic Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('critic loss')\n",
        "plt.show()\n",
        "\n",
        "# show zero-sum\n",
        "plt.plot(critic_losses, generator_losses)\n",
        "plt.title('Critic Loss vs. Generator Loss')\n",
        "plt.xlabel('generator loss')\n",
        "plt.ylabel('critic loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mkdir('saved_generators/')\n",
        "models.save_model(generator, f'saved_generators/{dataset_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Turn training progress images into gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "J9oY7nsFfvxZ"
      },
      "outputs": [],
      "source": [
        "skip_size = 5\n",
        "\n",
        "with imageio.get_writer(f'gifs/{dataset_name}.gif', mode='I') as writer:\n",
        "  filenames = sorted(glob.glob(f'output_images/{dataset_name}/image*.png'))\n",
        "  for i in range(0, len(filenames), skip_size):\n",
        "    writer.append_data(imageio.imread(filenames[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = models.load(f'saved_generators/greek_coins')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "-hHfK2GpWyOW",
        "outputId": "6b60292b-6065-4f86-90f8-bdd629c2310c"
      },
      "outputs": [],
      "source": [
        "num_examples = 36\n",
        "\n",
        "noise = tf.random.normal([num_examples, noise_dim])\n",
        "generated = generator(noise)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "for i in range(num_examples):\n",
        "  grid_size = int(np.ceil(np.sqrt(num_examples)))\n",
        "  fig.add_subplot(grid_size, grid_size, i + 1)\n",
        "  plt.imshow(generated[i] * 0.5 + 0.5)\n",
        "  plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0vQYdO-0m5JF"
      ],
      "machine_shape": "hm",
      "name": "GAN Fun",
      "provenance": []
    },
    "interpreter": {
      "hash": "e7e0b588e0cc6e5aa6734c2bf43dc39574292059249226e4a2513995b8f64877"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
